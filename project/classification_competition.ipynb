{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Text Classification Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project: Given a Tweet and context about the Tweet (from Twitter), predict whether the writer wrote the message sarcastically. The dataset may be found in here https://github.com/CS410Fall2020/ClassificationCompetition. In the train dataset, have the following columns as taken directly from the above link's README file:\n",
    "- response: the Tweet to be classified\n",
    "- context: the conversation context of the response\n",
    "- label: SARCASM or NOT_SARCASM (binary label, not present in the test data)\n",
    "- id: String identifier for sample. This id will be required when making submissions. (ONLY in test data)\n",
    "\n",
    "The train dataset has 5000 rows while the test data has 1800."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What have I tried?\n",
    "\n",
    "This file in particular will only show my results using BERT. I have tried other methods too, but things quickly got really out of hand so I won't attach the results here. Before using BERT, I have tried linear SVM models, using TFIDF vectorization. I also tried logistic regression and random forest. For the linear models, I used LASSO to determine which feature/words to use for my models, hoping to get something that is interpretable. None of these models surpassed the benchmark, but got around 0.67 for F1 scores. I then moved onto other methods such as CNNSVM, CNNLSTM, LSTMSVM, CNNLSTMSVM and a bunch of similar combinations using embeddings based off of GloVe's Twitter collection. I also tried using tokenization for preprocessing. For the features, I tried different combinations of splitting up responses and contexts and inserting them in different parts of the prediction algorithm (for example use CNN to generate features for the context data and then use TFIDF for the response data with LASSO and then merge the results together as the input feature to an SVM). I even tried doing weighted averages for predictions made separately on context and response. The best I could come up with was an F1 score of around 0.70.\n",
    "\n",
    "### What worked?\n",
    "\n",
    "Nothing seemed to work, so I started looking around at other methods. I came across BERT, a transformer-based machine learning technique pre-trained and developed by Google (https://arxiv.org/abs/1810.04805). It was created using a neural network architecture that you can read more about. BERT was trained on a very large corpus of unlabelled text, including Wikipedia. For the purposes of this project, I simply had to take this pretrained model and \"tune it\" according to my task of sarcasm prediction (classification problem). I did not have to experiment with different hyperparamters as the results of the first run already beat the baseline by a large margin. You can verify the results in the LiveDataLab competition webpage and look for the name Zinkoy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using Python 3.6.9. This probably works as long as you are using some flavor of Python 3. You can clone the repository yourself. Your directory should contain classification_competition.ipynb, requirements.txt and two folders called data and model. Run the following commands in your terminal. I am using Windows Linux Subsystem.\n",
    "#### Creating and setting up a virtual environment with Jupyter Notebook\n",
    "```\n",
    "$ python3.6 -m venv env\n",
    "$ source env/bin/activate\n",
    "$ pip install --upgrade pip\n",
    "$ pip install -r requirements.txt\n",
    "$ python -m ipykernel install --user --name=<whatever_name_you_want_without_the_brackets>\n",
    "$ jupyter notebook\n",
    "```\n",
    "At this point, copy and paste whatever url you get from running `jupyter notebook` into your favorite browser. Click on classification_competition.ipynb. Now click on the Kernel tab and change the kernel to whatever you named it to be. Follow the instructions in \"Usage Documentation\".\n",
    "When you are done testing, you can delete all of the files and then remove the Jupyter kernel by running \n",
    "\n",
    "`jupyter kernelspec uninstall <whatever_name_you_want_without_the_brackets>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "There will be a file called documentation.html, which is just this notebook, but in an easily accessible format. I would rather keep all of the documentation in one spot. To actually run the code, make sure you are looking at classification_competition.ipynb.\n",
    "### Overview of functions and implementation documentation\n",
    "These are both combined in this file. Before each major step, there will be an explanation of each function and its implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage Documentation\n",
    "\n",
    "Make sure that in the directory containing this Jupyter Notebook, you have a folder called data containing train.jsonl and test.jsonl obtained from the above github link. If you want the full experience, simply run all of the cells. It is normal for the model training process to take many hours and render your computer unusable for that period of time. If you want to just load in the model and output a submission, run all of the cells up until the section \"Training for BERT\". Then run the section \"Preparing our test data\". After that, you can run everything in the \"Express testing from a pretrained model\" section to generate answer.txt file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contribution of team members breakdown\n",
    "I am the team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages\n",
    "\n",
    "We will be using numpy and pandas to work with data. NLTK will be used for preprocessing. sklearn will help us with creading a train and test split for validation. Keras will be used alongside Tensorflow for defining things like loss. The main star will be the transformers which contain the BERT model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from transformers import *\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "The purpose of this section is to define functions to prepare out data in order to use BERT.\n",
    "\n",
    "The preprocess function takes in a Pandas dataframe as it's argument and outputs a new Pandas dataframe based off the original. The function creates a deep copy of our original dataframe. We then define a list of stopwords in the English langauge (excluding stopwords in the excluded_stop_words list). The stopwords that we don't remove are chosen based off of what I personally think would be important for understanding a sentence i.e. words like no/not can really completely change the meaning of a sentence. For every response inside of our dataframe we:\n",
    "1. Replace punctuations and non-numeric/alphabetic symbols with an empty string. We keep the symbols such as -, ', and #. \n",
    "2. Split up the sentence by whitespace into individual words\n",
    "3. Remove all stopwords other than those in the excluded_stop_words list. We also remove the words \"user\" and \"url\" since they appear everywhere and seem to sometimes cause overfitting.\n",
    "4. Convert each word to lowercase.\n",
    "5. Join back the sentence together by a normal space.\n",
    "\n",
    "Context is slightly different since it contains an array of Tweets. We simply join together every tweet by a space and then repeat what we did for response.\n",
    "\n",
    "We name these processed columns response_proc and context_proc respectively and return the new dataframe containing these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input: A pandas dataframe df. This is assumed to be the dataframe used in the sarcasm detection classification competition.\n",
    "The expected columns are \"response\" and \"context\"\n",
    "Output: A new pandas dataframe containing columns \"response_proc\" and \"context_proc\".\n",
    "'''\n",
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    excluded_stop_words = set([\"not\", \"most\",\"no\", \"off\", \"down\", \"any\", \"over\", \"under\", \"few\", \"against\", \"above\", \"below\"])\n",
    "    \n",
    "    df[\"response_proc\"] = df[\"response\"].str.replace('[^\\w\\s\\-\\'#]','').apply(\n",
    "            lambda doc : doc.split()).apply(\n",
    "            lambda doc : \" \".join([word.lower() for word in doc if (word.lower() in excluded_stop_words or (word.lower() not in stop_words and word.lower() not in ('user', 'url')))]))\n",
    "    \n",
    "        \n",
    "    df[\"context_proc\"] = df[\"context\"].str.join(\" \").str.replace('[^\\w\\s\\-\\'#]','').apply(\n",
    "        lambda doc : doc.split()).apply(\n",
    "        lambda doc : \" \".join([word.lower() for word in doc if (word.lower() in excluded_stop_words or (word.lower() not in stop_words and word.lower() not in ('user', 'url')))]))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The add_label function creates a deep copy of our original dataframe. We do this deep copying because for some reason, our original dataframe keeps getting modified in ways I do not like while testing. Deep copying prevents that issue. We create a column \"y\" containing values $\\{0, 1\\}$. We map the label \"SARCASM\" to 1 and \"NOT_SARCASM\" to 0. BERT and a bunch of other machine learning algorithms need labels in this form. We then return the new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input: A pandas dataframe df that has labels \"SARCASM\" and \"NOT_SARCASM\".\n",
    "Output: Returns a new dataframe containing the column \"y\" of mapped numerical label values.\n",
    "'''\n",
    "def add_label(df):\n",
    "    df = df.copy()\n",
    "    df[\"y\"] = pd.Categorical(df[\"label\"]).map({\"SARCASM\":1, \"NOT_SARCASM\":0}).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is a very simple function. It must be called after the preprocess(df) function. We take in a dataframe and create a deepcopy expecting to see the columns \"response_proc\" and \"context_proc\" (created from preprocess(df)). We then combine them in a new column \"combined_proc\" by a simple space. An important caveat to notice is that append our processed context at the end of our processed response. This heuristically allows our response to never fully be cut off later on. We can set the number of tokens we want BERT to be able to encode, but at the same time can't set that number to be too high due to computational constraints. The design choice here may seem weird (why not keep these lines in preprocess(df)?), but it is useful for experimenting different combinations of text for different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input: A pandas dataframe df that has successfully went through the function \"preprocess(df)\"\n",
    "Output: A new dataframe containing the combination of processed response and context \"combined_proc\"\n",
    "'''\n",
    "def combine_context_response(df):\n",
    "    df = df.copy()\n",
    "    df['combined_proc'] = df[['response_proc', 'context_proc']].apply(lambda text: ' '.join(text), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the data from https://github.com/CS410Fall2020/ClassificationCompetition/tree/main/data and load in the training dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>response</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SARCASM</td>\n",
       "      <td>@USER @USER @USER I don't get this .. obviousl...</td>\n",
       "      <td>[A minor child deserves privacy and should be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SARCASM</td>\n",
       "      <td>@USER @USER trying to protest about . Talking ...</td>\n",
       "      <td>[@USER @USER Why is he a loser ? He's just a P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SARCASM</td>\n",
       "      <td>@USER @USER @USER He makes an insane about of ...</td>\n",
       "      <td>[Donald J . Trump is guilty as charged . The e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SARCASM</td>\n",
       "      <td>@USER @USER Meanwhile Trump won't even release...</td>\n",
       "      <td>[Jamie Raskin tanked Doug Collins . Collins lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SARCASM</td>\n",
       "      <td>@USER @USER Pretty Sure the Anti-Lincoln Crowd...</td>\n",
       "      <td>[Man ... y ’ all gone “ both sides ” the apoca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                           response  \\\n",
       "0  SARCASM  @USER @USER @USER I don't get this .. obviousl...   \n",
       "1  SARCASM  @USER @USER trying to protest about . Talking ...   \n",
       "2  SARCASM  @USER @USER @USER He makes an insane about of ...   \n",
       "3  SARCASM  @USER @USER Meanwhile Trump won't even release...   \n",
       "4  SARCASM  @USER @USER Pretty Sure the Anti-Lincoln Crowd...   \n",
       "\n",
       "                                             context  \n",
       "0  [A minor child deserves privacy and should be ...  \n",
       "1  [@USER @USER Why is he a loser ? He's just a P...  \n",
       "2  [Donald J . Trump is guilty as charged . The e...  \n",
       "3  [Jamie Raskin tanked Doug Collins . Collins lo...  \n",
       "4  [Man ... y ’ all gone “ both sides ” the apoca...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_json(\"data/train.jsonl\", lines=True)\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the same link above, we download and load in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json(\"data/test.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkout the distribution of labels in our train set. Looks balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "NOT_SARCASM    2500\n",
       "SARCASM        2500\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby([\"label\"]).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare our train data for BERT using the functions we defined earlier. We extract the \"y\" column and make sure that it is a numpy array of the correct shape for BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prepped = add_label(combine_context_response(preprocess(train)))\n",
    "train_y = train_prepped[\"y\"].to_numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>response</th>\n",
       "      <th>context</th>\n",
       "      <th>response_proc</th>\n",
       "      <th>context_proc</th>\n",
       "      <th>combined_proc</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SARCASM</td>\n",
       "      <td>@USER @USER @USER I don't get this .. obviousl...</td>\n",
       "      <td>[A minor child deserves privacy and should be ...</td>\n",
       "      <td>get obviously care would've moved right along ...</td>\n",
       "      <td>minor child deserves privacy kept politics pam...</td>\n",
       "      <td>get obviously care would've moved right along ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SARCASM</td>\n",
       "      <td>@USER @USER trying to protest about . Talking ...</td>\n",
       "      <td>[@USER @USER Why is he a loser ? He's just a P...</td>\n",
       "      <td>trying protest talking labels label wtf make em</td>\n",
       "      <td>loser he's press secretary make excuses crowd ...</td>\n",
       "      <td>trying protest talking labels label wtf make e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SARCASM</td>\n",
       "      <td>@USER @USER @USER He makes an insane about of ...</td>\n",
       "      <td>[Donald J . Trump is guilty as charged . The e...</td>\n",
       "      <td>makes insane money movies einstein #learnhowth...</td>\n",
       "      <td>donald j trump guilty charged evidence clear s...</td>\n",
       "      <td>makes insane money movies einstein #learnhowth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SARCASM</td>\n",
       "      <td>@USER @USER Meanwhile Trump won't even release...</td>\n",
       "      <td>[Jamie Raskin tanked Doug Collins . Collins lo...</td>\n",
       "      <td>meanwhile trump even release sat scores wharto...</td>\n",
       "      <td>jamie raskin tanked doug collins collins looks...</td>\n",
       "      <td>meanwhile trump even release sat scores wharto...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SARCASM</td>\n",
       "      <td>@USER @USER Pretty Sure the Anti-Lincoln Crowd...</td>\n",
       "      <td>[Man ... y ’ all gone “ both sides ” the apoca...</td>\n",
       "      <td>pretty sure anti-lincoln crowd claimed democra...</td>\n",
       "      <td>man gone sides apocalypse one day already obam...</td>\n",
       "      <td>pretty sure anti-lincoln crowd claimed democra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                           response  \\\n",
       "0  SARCASM  @USER @USER @USER I don't get this .. obviousl...   \n",
       "1  SARCASM  @USER @USER trying to protest about . Talking ...   \n",
       "2  SARCASM  @USER @USER @USER He makes an insane about of ...   \n",
       "3  SARCASM  @USER @USER Meanwhile Trump won't even release...   \n",
       "4  SARCASM  @USER @USER Pretty Sure the Anti-Lincoln Crowd...   \n",
       "\n",
       "                                             context  \\\n",
       "0  [A minor child deserves privacy and should be ...   \n",
       "1  [@USER @USER Why is he a loser ? He's just a P...   \n",
       "2  [Donald J . Trump is guilty as charged . The e...   \n",
       "3  [Jamie Raskin tanked Doug Collins . Collins lo...   \n",
       "4  [Man ... y ’ all gone “ both sides ” the apoca...   \n",
       "\n",
       "                                       response_proc  \\\n",
       "0  get obviously care would've moved right along ...   \n",
       "1    trying protest talking labels label wtf make em   \n",
       "2  makes insane money movies einstein #learnhowth...   \n",
       "3  meanwhile trump even release sat scores wharto...   \n",
       "4  pretty sure anti-lincoln crowd claimed democra...   \n",
       "\n",
       "                                        context_proc  \\\n",
       "0  minor child deserves privacy kept politics pam...   \n",
       "1  loser he's press secretary make excuses crowd ...   \n",
       "2  donald j trump guilty charged evidence clear s...   \n",
       "3  jamie raskin tanked doug collins collins looks...   \n",
       "4  man gone sides apocalypse one day already obam...   \n",
       "\n",
       "                                       combined_proc  y  \n",
       "0  get obviously care would've moved right along ...  1  \n",
       "1  trying protest talking labels label wtf make e...  1  \n",
       "2  makes insane money movies einstein #learnhowth...  1  \n",
       "3  meanwhile trump even release sat scores wharto...  1  \n",
       "4  pretty sure anti-lincoln crowd claimed democra...  1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_prepped.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for submission\n",
    "\n",
    "This section might seem to have come early, but I simply took this from another file that I was working on, so I might as well place it here as it will be useful for testing later on.\n",
    "\n",
    "create_submission takes in our original test dataset and prediction values for each element. Each row in the test dataset corresponds directly to each prediction value (i.e. we don't have to worry about ordering when creating a submission dataframe). We create a new submission dataframe based off of our predictions and then append the test[\"id\"] column to it. We rename the columns to \"id\" and \"pred\" for internal record keeping. We then create a new column \"label\" and map predictions values of 1 to \"SARCASM\" and 0 to \"NOT_SARCASM\". We then drop the \"pred\" column since we don't need it for our submission. We return this new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input: The original dataframe test and numpy array of (0,1) predictions preds\n",
    "Output: A new dataframe containing the test ID and corresponding prediction in terms of \"SARCASM\" and \"NOT_SARCASM\"\n",
    "'''\n",
    "def create_submission(test, preds):\n",
    "    mapping = {1: \"SARCASM\", 0: \"NOT_SARCASM\"}\n",
    "    df_preds = pd.DataFrame(preds)\n",
    "    sub_df = pd.concat([test[\"id\"], df_preds], axis=1)\n",
    "    sub_df.columns = [\"id\", \"pred\"]\n",
    "    sub_df[\"label\"] = sub_df[\"pred\"].map(mapping)\n",
    "    sub_df = sub_df.drop([\"pred\"], axis=1)\n",
    "    return sub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output_submission takes in the dataframe created in create_submission. It simply writes the results of into a .txt file with no indices nor headers, i.e. a file with just test ids and the corresponding prediction delimited by a comma, with each pair being on a newline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input: A dataframe created from create_submission\n",
    "Output: Returns nothing, but creates a file answer.txt of test ids and predicted labels\n",
    "'''\n",
    "def output_submission(sub_df):\n",
    "    sub_df.to_csv('answer.txt', index=None, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing for BERT\n",
    "\n",
    "Disclaimer: I used https://swatimeena989.medium.com/bert-text-classification-using-keras-903671e0207d to help me learn how to setup BERT.\n",
    "\n",
    "We first define the number of classes we need. In order to do BinaryCrossentropy later on for BERT's loss function, set the number of classes to 1. Having it at 2 apparently doesn't play well. We then load in the standard sized-pretrained BERT tokenizer fitted on lowercased tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 1\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a list by running the bert_tokenizer on every document/tweet block in our combined processed training dataset. We allow for the tokenizer to do its magic by adding special tokens, set the max_length for number of tokens to 256, and then pad each document to fit the max_length for consistency. Ignore the warning below. Each tokenized object will contain \"input_ids\" and \"attention_mask\" in a dictionary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/mnt/c/Users/bvien2/Documents/UIUC/cs_410/ClassificationCompetition/test/env/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "combined_data = [bert_tokenizer.encode_plus(doc, add_special_tokens = True, max_length=256, pad_to_max_length = True) for doc in train_prepped[\"combined_proc\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a look at the first element's \"input_ids\" to see exactly what happened. It looks like we have some tokens and then the words of the Tweet block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] get obviously care would've moved right along instead decided care troll minor child deserves privacy kept politics pamela karlan ashamed angry obviously biased public pandering using child child named barron # bebest melania care less fact [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.decode(combined_data[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use BERT, we must extract the \"input_ids\" and \"attention_masks\" from the list we just created by bert_tokenizer. We then convert both arrays into Numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = np.array([datapoint[\"input_ids\"] for datapoint in combined_data])\n",
    "masks = np.array([datapoint[\"attention_mask\"] for datapoint in combined_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split our dataset into train and validation components. 20% of the data will be used for validation, i.e. 4000 train and 1000 validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids,val_ids,train_split_y,val_y,train_mask,val_mask = train_test_split(input_ids, train_y, masks,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the BERT classification model for uncased tokens with our defined number of classes. Once again, ignore the warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertForSequenceClassification: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier', 'dropout_37']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training BERT\n",
    "\n",
    "We define our loss function to be BinaryCrossentropy since we have binary labels. Since we are interested in monitoring our F1 score, we import it from Tensorflow to plug into our BERT model. We will be using Adam for our optimization algorithm used in training with a learning rate of 2e-5 and an epsilon of 1e-06. We then compile these settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metric = tfa.metrics.F1Score(num_classes=n_classes)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5,epsilon=1e-06)\n",
    "bert_model.compile(loss=loss,optimizer=optimizer,metrics=[metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the model. This process takes around 8+ hours, so I will not be doing this portion again. Interestingly, our F1 score doesn't seem to be improving while loss decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "157/157 [==============================] - 6796s 43s/step - loss: 0.5525 - f1_score: 0.6667 - val_loss: 0.3924 - val_f1_score: 0.6693\n",
      "Epoch 2/4\n",
      "157/157 [==============================] - 6764s 43s/step - loss: 0.4128 - f1_score: 0.6667 - val_loss: 0.2872 - val_f1_score: 0.6693\n",
      "Epoch 3/4\n",
      "157/157 [==============================] - 6825s 43s/step - loss: 0.2812 - f1_score: 0.6667 - val_loss: 0.0971 - val_f1_score: 0.6693\n",
      "Epoch 4/4\n",
      "157/157 [==============================] - 6722s 43s/step - loss: 0.1399 - f1_score: 0.6667 - val_loss: 0.0473 - val_f1_score: 0.6693\n"
     ]
    }
   ],
   "source": [
    "fitted = bert_model.fit([input_ids, masks], train_y,batch_size=32,epochs=4, validation_data=([val_ids,val_mask],val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing our test data\n",
    "\n",
    "We preprocess our test data like how we processed our training data, minus the part where we add labels since they don't exist here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prepped = combine_context_response(preprocess(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then make sure it gets tokenized exactly how our train data was tokenized and then extract the \"input_ids\"/\"attention_mask\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_test = [bert_tokenizer.encode_plus(doc, add_special_tokens = True, max_length=256, pad_to_max_length = True) for doc in test_prepped[\"combined_proc\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = np.array([datapoint[\"input_ids\"] for datapoint in combined_test])\n",
    "test_masks = np.array([datapoint[\"attention_mask\"] for datapoint in combined_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on test data\n",
    "\n",
    "We first run BERT's prediction function. This function will also take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = bert_model.predict([test_ids, test_masks], batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results seemed to output and array with both negative and positive values. We map each value greater than or equal to 0 to the label 1 (\"SARCASM\") and values less than 0 with 0 (\"NOT_SARCASM\"). We make sure the array is a one-dimensional numpy array so it's easy to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = (np.array(preds) >= 0).reshape(-1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputting our submission\n",
    "\n",
    "We will create our submission output here as by how we defined the process in the \"Prepare data for submission\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = create_submission(test, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>twitter_1</td>\n",
       "      <td>SARCASM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>twitter_2</td>\n",
       "      <td>SARCASM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>twitter_3</td>\n",
       "      <td>SARCASM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>twitter_4</td>\n",
       "      <td>SARCASM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>twitter_5</td>\n",
       "      <td>SARCASM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    label\n",
       "0  twitter_1  SARCASM\n",
       "1  twitter_2  SARCASM\n",
       "2  twitter_3  SARCASM\n",
       "3  twitter_4  SARCASM\n",
       "4  twitter_5  SARCASM"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the distribution of predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "NOT_SARCASM     628\n",
       "SARCASM        1172\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df.groupby([\"label\"], axis=0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_submission(sub_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving our BERT model\n",
    "\n",
    "This will allow us to hopefully load in the model again later on without the 8+ hours training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.save_pretrained(\"./model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Express testing from a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in our BERT model\n",
    "\n",
    "This is basically everything that goes down in the sections \"Training BERT\" and \"Prediction on test data\". This will take quite a bit of time to run. Ignore the warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertForSequenceClassification: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier', 'dropout_75']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=n_classes)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metric = tfa.metrics.F1Score(num_classes=n_classes)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5,epsilon=1e-06)\n",
    "bert_model.compile(loss=loss,optimizer=optimizer,metrics=[metric])\n",
    "bert_model.load_weights(\"./model/tf_model.h5\")\n",
    "\n",
    "preds = bert_model.predict([test_ids, test_masks], batch_size=32)\n",
    "test_y = (np.array(preds) >= 0).reshape(-1).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then output our submission in the same exact way as \"Outputting our submission\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = create_submission(test, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "NOT_SARCASM     628\n",
       "SARCASM        1172\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df.groupby([\"label\"], axis=0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_submission(sub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
